#### Semester 3: Foundational ML, Subspaces, and Integration

| Project Idea | Core Pillars Demonstrated | Description & Source Justification |
| :--- | :--- | :--- |
| **1. Column Space and Solubility** | LA, Python | Analyze a matrix $A$ and a vector $\mathbf{b}$. Determine whether $\mathbf{b}$ lies in the column space $\mathbf{C}(A)$ by attempting to solve $\mathbf{A} \mathbf{x} = \mathbf{b}$. Conclude if the system is solvable based on whether $\mathbf{b}$ is a linear combination of the columns [53-55]. |
| **2. K-Nearest Neighbors (KNN) Classifier** | ML/DS, Python | Implement the KNN algorithm using `scikit-learn` to classify a set of data points (e.g., the Iris dataset). Evaluate the performance using a simple train/test split and report accuracy [56-59]. |
| **3. Confidence Sets Construction** | Stats, Python | Construct and interpret a $1-\alpha$ confidence interval for the population mean $\mu$ for a real-world dataset, demonstrating the concept of a confidence set as an interval that traps the true value with a given frequency [60-62]. |
| **4. Introduction to Integration** | Calculus, Python | Use numerical integration (e.g., trapezoidal rule, implemented in Python from scratch or via SciPy) to estimate the area under the curve of a non-elementary function over a given interval, reflecting the geometric definition of the integral [63-66]. |
| **5. Time-Series Resampling Analysis** | Python/DS | Load a simple time-series dataset. Use Pandas functionality (`pd.to_datetime`, `resample`) to change the frequency of the data (e.g., daily to weekly) and calculate rolling averages or moving window functions, addressing practical data constraints [67-69]. |
| **6. Simple Least Squares Fitting** | LA, Stats, Python | Given an overdetermined system ($\mathbf{A} \mathbf{x} \approx \mathbf{b}$), use the least squares method ($\mathbf{A}^T \mathbf{A} \mathbf{x} = \mathbf{A}^T \mathbf{b}$) to find the best approximate solution $\hat{\mathbf{x}}$. Use NumPy to solve the resulting matrix equation [70, 71]. |
| **7. Polynomial Regression Model Selection** | Stats, Python/DS | Fit polynomial regression models of different degrees (e.g., degree 1 to 5) to a simulated dataset. Use a cross-validation method (e.g., LOOCV or $k$-fold cross-validation) to select the optimal model complexity, analyzing the trade-off between bias and variance [72-75]. |
| **8. Non-Linear Curve Fitting** | Calculus, Python | Use derivatives to analyze a rational or complex function (e.g., $f(x) = \sin(x^2)$). Calculate the derivatives using the chain rule and find the extrema, visualizing the results alongside the original function [76, 77]. |
